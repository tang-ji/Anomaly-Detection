{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tree_regularization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "sK4iOYXqdtwC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ==========================================================\n",
        "## You should first upload tree_regularization.py and DataLoader.py in the left control bar\n",
        "## =========================================================="
      ]
    },
    {
      "metadata": {
        "id": "tYk0c25oY1iD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download and preprocess dataset"
      ]
    },
    {
      "metadata": {
        "id": "Z_NsTHD09coE",
        "colab_type": "code",
        "outputId": "dbf68cab-7a79-4def-acf4-2c0ef581bccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "import zipfile\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file = '1Nl59uCt3SeCMH891nwS6I3vJbDrVKXQm' #-- Updated File ID \n",
        "file2 = '1JIw9Kc2K7W_sJaMoZOavGicuf7qrPFBr'\n",
        "file3 = '1-P6lzpwyXxet37NwJCNhxm_dwZ22Gm42'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 20.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UH0Vix_tdq_x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## You need to click the link and get the authorization of Google drive in the box above"
      ]
    },
    {
      "metadata": {
        "id": "pyU6AsMIXSv3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dataset unzip"
      ]
    },
    {
      "metadata": {
        "id": "S9gXAqGn9gw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "downloaded = drive.CreateFile({'id': file})\n",
        "downloaded.GetContentFile('zip.zip') \n",
        "!unzip -q 'zip.zip'\n",
        "downloaded = drive.CreateFile({'id': file2})\n",
        "downloaded.GetContentFile('zip.zip') \n",
        "!unzip -q 'zip.zip'\n",
        "downloaded = drive.CreateFile({'id': file3})\n",
        "downloaded.GetContentFile('zip.zip') \n",
        "!unzip -q 'zip.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xJBZHAmlXXdY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set custom dataset path"
      ]
    },
    {
      "metadata": {
        "id": "iCcI2aZtLmGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cpu_contention_path = 'cpu_contention/'\n",
        "input_rate_path = 'labeled_process_A/input_rate/'\n",
        "node_failure_path = 'labeled_process_A/node_failure/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tv062gSAXaAq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading data for 3 different abnormal types"
      ]
    },
    {
      "metadata": {
        "id": "0WaprgH2XiEc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "f0058955-ab52-479e-e466-6db0e5174707"
      },
      "cell_type": "code",
      "source": [
        "from DataLoader import DataLoader\n",
        "abnormal_cpu_contention, abnormal_input_rate, abnormal_node_failure, normal_cpu_contention, normal_input_rate, normal_node_failure = DataLoader(cpu_contention_path, input_rate_path, node_failure_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DataLoader.py:31: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  abnormal_cpu_contention = abnormal_cpu_contention.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:33: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  normal_cpu_contention = normal_cpu_contention.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:45: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  abnormal_input_rate = abnormal_input_rate.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:47: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  normal_input_rate = normal_input_rate.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:59: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  abnormal_node_failure = abnormal_node_failure.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:61: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  normal_node_failure = normal_node_failure.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:31: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  abnormal_cpu_contention = abnormal_cpu_contention.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:33: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  normal_cpu_contention = normal_cpu_contention.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:45: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  abnormal_input_rate = abnormal_input_rate.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:47: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  normal_input_rate = normal_input_rate.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:59: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  abnormal_node_failure = abnormal_node_failure.reindex_axis(header, axis=1)\n",
            "/content/DataLoader.py:61: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
            "  normal_node_failure = normal_node_failure.reindex_axis(header, axis=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "w8oAuHpIXkMO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get samples for each abnormal type and normal type"
      ]
    },
    {
      "metadata": {
        "id": "9yBQhVc8-MXI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "abnormal = pd.concat((abnormal_cpu_contention.sample(n=2500), abnormal_input_rate.sample(n=2500), abnormal_node_failure))\n",
        "normal = pd.concat((normal_cpu_contention.sample(n=2500), normal_input_rate.sample(n=2500), normal_node_failure.sample(n=2500)))\n",
        "\n",
        "abnormal = abnormal.sample(frac=1)\n",
        "normal = normal.sample(frac=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gKy0n1GvX0_J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uncomment to set only two labels \"normal\" and \"abnormal\""
      ]
    },
    {
      "metadata": {
        "id": "MgU52lhKX9Mr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "abnormal['label'] = 'abnormal'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M5SwbGhkX_mx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get custom data proportion"
      ]
    },
    {
      "metadata": {
        "id": "k0AuqYLN-OdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_test = pd.concat((abnormal.iloc[:250, :], normal.iloc[5000:7500, :]))\n",
        "df = pd.concat((abnormal.iloc[500:4500, :], normal.iloc[:4000, :]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-8o6S2gYbuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Shuffle and fill Nan value"
      ]
    },
    {
      "metadata": {
        "id": "LFqacYeSAt0p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1)\n",
        "df = df.fillna(value = 0)\n",
        "df_test = df_test.fillna(value = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJEsjNEtZEha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Encode categories into numbers"
      ]
    },
    {
      "metadata": {
        "id": "VYHdJsmeAvtq",
        "colab_type": "code",
        "outputId": "ad15b261-9d04-40eb-b401-4fc6f6c34fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cat = set(df['label'])\n",
        "cat_index = dict(zip(cat, range(len(cat))))\n",
        "index_cat = dict(zip(range(len(cat)), cat))\n",
        "cat_index"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abnormal': 0, 'normal': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abnormal': 0, 'normal': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "H_b1k3hhZMuI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Release RAM space"
      ]
    },
    {
      "metadata": {
        "id": "eFgJwPLWAymH",
        "colab_type": "code",
        "outputId": "8448b9cf-5e13-4c5d-da71-c32ddb6dc679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del abnormal_cpu_contention, abnormal_input_rate, abnormal_node_failure\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "YnttNIbbZuiW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Drop columns \"t\", \"comment\" and \"label\""
      ]
    },
    {
      "metadata": {
        "id": "AN_K_oPsAz60",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y = np.array(df['label'].map(lambda x: cat_index[x]))\n",
        "df = df.drop(['t', 'label', 'comment'], axis=1)\n",
        "Y_test = np.array(df_test['label'].map(lambda x: cat_index[x]))\n",
        "df_test = df_test.drop(['t', 'label', 'comment'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5CuLCNIZzW_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Value scaling"
      ]
    },
    {
      "metadata": {
        "id": "6d0FFHeUZ10Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(df)\n",
        "X_test = scaler.transform(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSFkq3UiaYIg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uncomment to use RandomOverSampler\n",
        "\n",
        "* If we don't use this and our training set is not blance, our model is going to \"cheat\" --- Always predicting normal to get good score"
      ]
    },
    {
      "metadata": {
        "id": "oc5IIYtbQnsy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X, Y = ros.fit_resample(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xX3yD8mqnZWn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Transform to one hot vectors"
      ]
    },
    {
      "metadata": {
        "id": "BFtjPfdknYTT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y = np.eye(len(cat))[Y.reshape(-1)]\n",
        "Y_test = np.eye(len(cat))[Y_test.reshape(-1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gti3c1FcA-Ll",
        "colab_type": "code",
        "outputId": "f51d484e-0e00-4162-f314-2b7d91b1b83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2750"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2750"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "C9hFtV4aeoT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tree regularization"
      ]
    },
    {
      "metadata": {
        "id": "2A3oiHwwfr_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "class TreeMLP(in_count, out_count, hidden_sizes, strength)\n",
        "\n",
        "in_count: input shape\n",
        "\n",
        "out_count: output shape\n",
        "\n",
        "hidden_sizes: surrogate network's structure\n",
        "\n",
        "strength: regularization strengh\n",
        "\n",
        "APL_mode: \n",
        "\n",
        "0 - implemented by paper's source code\n",
        "\n",
        "1 - implemented by its definition"
      ]
    },
    {
      "metadata": {
        "id": "rLHkXKzCHeHw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tree_regularization import *\n",
        "APL_mode = 0\n",
        "t = TreeMLP(X.shape[1], len(cat), [20, 50, 50, 50], strength=0.01, APL_mode=APL_mode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TV5t1Bl3h18I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the next cell, the tree regularization network will take at least 1 hour to get stable prediction"
      ]
    },
    {
      "metadata": {
        "id": "fjS8ggaagM2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8569
        },
        "outputId": "7b1f8bbd-d99c-4200-cf2b-c227a6f0a6df"
      },
      "cell_type": "code",
      "source": [
        "t.train(X, Y, batch_size=128, iters_retrain=5, epochs_sur=20, epochs_mlp=4, feature_names=list(df.columns.values), validation_data=(X_test, Y_test), class_names=list(cat_index.keys()))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training MLP net... [1/3]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 3s 360us/step - loss: 0.7039 - acc: 0.5042 - val_loss: 0.7629 - val_acc: 0.0909\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 77us/step - loss: 0.6889 - acc: 0.5970 - val_loss: 0.6883 - val_acc: 0.6360\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.5699 - acc: 0.7622 - val_loss: 0.3637 - val_acc: 0.9062\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.4193 - acc: 0.8366 - val_loss: 0.3437 - val_acc: 0.9004\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.3867 - acc: 0.8574 - val_loss: 0.3863 - val_acc: 0.8807\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 79us/step - loss: 0.3572 - acc: 0.8718 - val_loss: 0.3016 - val_acc: 0.9120\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 78us/step - loss: 0.3510 - acc: 0.8749 - val_loss: 0.3783 - val_acc: 0.8815\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 77us/step - loss: 0.3312 - acc: 0.8860 - val_loss: 0.3100 - val_acc: 0.9120\n",
            "Epoch 9/10\n",
            " 128/8000 [..............................] - ETA: 0s - loss: 0.4122 - acc: 0.8438 APL:14.51\n",
            " 256/8000 [..............................] - ETA: 2:22 - loss: 0.2878 - acc: 0.8984 APL:14.61\n",
            " 384/8000 [>.............................] - ETA: 2:53 - loss: 0.2973 - acc: 0.9036 APL:14.63\n",
            " 512/8000 [>.............................] - ETA: 3:06 - loss: 0.3175 - acc: 0.8906 APL:14.21\n",
            " 640/8000 [=>............................] - ETA: 3:11 - loss: 0.3129 - acc: 0.8922 APL:14.50\n",
            " 768/8000 [=>............................] - ETA: 3:14 - loss: 0.3238 - acc: 0.8867 APL:14.69\n",
            " 896/8000 [==>...........................] - ETA: 3:14 - loss: 0.3101 - acc: 0.8929 APL:14.47\n",
            "1024/8000 [==>...........................] - ETA: 3:14 - loss: 0.3021 - acc: 0.8955 APL:14.21\n",
            "1152/8000 [===>..........................] - ETA: 3:12 - loss: 0.3054 - acc: 0.8958 APL:14.60\n",
            "1280/8000 [===>..........................] - ETA: 3:10 - loss: 0.3082 - acc: 0.8961 APL:14.60\n",
            "1408/8000 [====>.........................] - ETA: 3:08 - loss: 0.3129 - acc: 0.8942 APL:14.64\n",
            "1536/8000 [====>.........................] - ETA: 3:05 - loss: 0.3155 - acc: 0.8926 APL:14.17\n",
            "1664/8000 [=====>........................] - ETA: 3:02 - loss: 0.3141 - acc: 0.8924 APL:14.47\n",
            "1792/8000 [=====>........................] - ETA: 2:59 - loss: 0.3128 - acc: 0.8917 APL:14.47\n",
            "1920/8000 [======>.......................] - ETA: 2:56 - loss: 0.3165 - acc: 0.8891 APL:14.46\n",
            "2048/8000 [======>.......................] - ETA: 2:53 - loss: 0.3131 - acc: 0.8911 APL:14.62\n",
            "2176/8000 [=======>......................] - ETA: 2:50 - loss: 0.3101 - acc: 0.8925 APL:14.81\n",
            "2304/8000 [=======>......................] - ETA: 2:46 - loss: 0.3124 - acc: 0.8919 APL:14.60\n",
            "2432/8000 [========>.....................] - ETA: 2:43 - loss: 0.3108 - acc: 0.8927 APL:14.59\n",
            "2560/8000 [========>.....................] - ETA: 2:40 - loss: 0.3084 - acc: 0.8934 APL:14.45\n",
            "2688/8000 [=========>....................] - ETA: 2:36 - loss: 0.3067 - acc: 0.8943 APL:14.43\n",
            "2816/8000 [=========>....................] - ETA: 2:33 - loss: 0.3082 - acc: 0.8942 APL:14.43\n",
            "2944/8000 [==========>...................] - ETA: 2:29 - loss: 0.3087 - acc: 0.8940 APL:14.61\n",
            "3072/8000 [==========>...................] - ETA: 2:26 - loss: 0.3047 - acc: 0.8962 APL:14.61\n",
            "3200/8000 [===========>..................] - ETA: 2:22 - loss: 0.3033 - acc: 0.8969 APL:14.59\n",
            "3328/8000 [===========>..................] - ETA: 2:18 - loss: 0.3064 - acc: 0.8963 APL:14.61\n",
            "3456/8000 [===========>..................] - ETA: 2:15 - loss: 0.3077 - acc: 0.8961 APL:14.55\n",
            "3584/8000 [============>.................] - ETA: 2:11 - loss: 0.3062 - acc: 0.8968 APL:14.58\n",
            "3712/8000 [============>.................] - ETA: 2:07 - loss: 0.3035 - acc: 0.8976 APL:14.62\n",
            "3840/8000 [=============>................] - ETA: 2:04 - loss: 0.3056 - acc: 0.8964 APL:14.67\n",
            "3968/8000 [=============>................] - ETA: 2:00 - loss: 0.3075 - acc: 0.8954 APL:14.62\n",
            "4096/8000 [==============>...............] - ETA: 1:56 - loss: 0.3046 - acc: 0.8970 APL:14.88\n",
            "4224/8000 [==============>...............] - ETA: 1:52 - loss: 0.3013 - acc: 0.8975 APL:14.61\n",
            "4352/8000 [===============>..............] - ETA: 1:49 - loss: 0.3004 - acc: 0.8980 APL:14.58\n",
            "4480/8000 [===============>..............] - ETA: 1:45 - loss: 0.3046 - acc: 0.8964 APL:14.21\n",
            "4608/8000 [================>.............] - ETA: 1:42 - loss: 0.3028 - acc: 0.8974 APL:14.61\n",
            "4736/8000 [================>.............] - ETA: 1:38 - loss: 0.3014 - acc: 0.8986 APL:14.61\n",
            "4864/8000 [=================>............] - ETA: 1:34 - loss: 0.3054 - acc: 0.8968 APL:14.61\n",
            "4992/8000 [=================>............] - ETA: 1:30 - loss: 0.3069 - acc: 0.8958 APL:14.60\n",
            "5120/8000 [==================>...........] - ETA: 1:26 - loss: 0.3065 - acc: 0.8957 APL:14.60\n",
            "5248/8000 [==================>...........] - ETA: 1:23 - loss: 0.3078 - acc: 0.8948 APL:14.60\n",
            "5376/8000 [===================>..........] - ETA: 1:19 - loss: 0.3073 - acc: 0.8955 APL:14.61\n",
            "5504/8000 [===================>..........] - ETA: 1:15 - loss: 0.3076 - acc: 0.8948 APL:14.21\n",
            "5632/8000 [====================>.........] - ETA: 1:11 - loss: 0.3086 - acc: 0.8947 APL:14.48\n",
            "5760/8000 [====================>.........] - ETA: 1:07 - loss: 0.3086 - acc: 0.8946 APL:12.21\n",
            "5888/8000 [=====================>........] - ETA: 1:03 - loss: 0.3083 - acc: 0.8945 APL:14.47\n",
            "6016/8000 [=====================>........] - ETA: 59s - loss: 0.3079 - acc: 0.8943  APL:14.60\n",
            "6144/8000 [======================>.......] - ETA: 56s - loss: 0.3071 - acc: 0.8944 APL:14.60\n",
            "6272/8000 [======================>.......] - ETA: 52s - loss: 0.3074 - acc: 0.8943 APL:14.61\n",
            "6400/8000 [=======================>......] - ETA: 48s - loss: 0.3085 - acc: 0.8938 APL:14.21\n",
            "6528/8000 [=======================>......] - ETA: 44s - loss: 0.3087 - acc: 0.8934 APL:14.43\n",
            "6656/8000 [=======================>......] - ETA: 40s - loss: 0.3077 - acc: 0.8938 APL:14.80\n",
            "6784/8000 [========================>.....] - ETA: 36s - loss: 0.3076 - acc: 0.8936 APL:14.74\n",
            "6912/8000 [========================>.....] - ETA: 33s - loss: 0.3067 - acc: 0.8940 APL:14.24\n",
            "7040/8000 [=========================>....] - ETA: 29s - loss: 0.3084 - acc: 0.8936 APL:14.62\n",
            "7168/8000 [=========================>....] - ETA: 25s - loss: 0.3075 - acc: 0.8944 APL:14.24\n",
            "7296/8000 [==========================>...] - ETA: 21s - loss: 0.3087 - acc: 0.8942 APL:14.25\n",
            "7424/8000 [==========================>...] - ETA: 17s - loss: 0.3077 - acc: 0.8948 APL:14.22\n",
            "7552/8000 [===========================>..] - ETA: 13s - loss: 0.3074 - acc: 0.8953 APL:14.62\n",
            "7680/8000 [===========================>..] - ETA: 9s - loss: 0.3080 - acc: 0.8948  APL:14.86\n",
            "7808/8000 [============================>.] - ETA: 5s - loss: 0.3071 - acc: 0.8955 APL:14.56\n",
            "7936/8000 [============================>.] - ETA: 1s - loss: 0.3067 - acc: 0.8957 APL:14.61\n",
            " APL:13.15\n",
            "8000/8000 [==============================] - 249s 31ms/step - loss: 0.3080 - acc: 0.8949 - val_loss: 0.2424 - val_acc: 0.9320\n",
            "Epoch 10/10\n",
            " 128/8000 [..............................] - ETA: 0s - loss: 0.3343 - acc: 0.8672 APL:12.75\n",
            " 256/8000 [..............................] - ETA: 1:53 - loss: 0.3051 - acc: 0.8945 APL:14.79\n",
            " 384/8000 [>.............................] - ETA: 2:32 - loss: 0.3189 - acc: 0.8906 APL:14.62\n",
            " 512/8000 [>.............................] - ETA: 2:49 - loss: 0.3244 - acc: 0.8887 APL:14.45\n",
            " 640/8000 [=>............................] - ETA: 2:58 - loss: 0.3180 - acc: 0.8891 APL:14.48\n",
            " 768/8000 [=>............................] - ETA: 3:02 - loss: 0.3093 - acc: 0.8867 APL:14.65\n",
            " 896/8000 [==>...........................] - ETA: 3:05 - loss: 0.2986 - acc: 0.8906 APL:14.63\n",
            "1024/8000 [==>...........................] - ETA: 3:06 - loss: 0.2887 - acc: 0.8975 APL:14.29\n",
            "1152/8000 [===>..........................] - ETA: 3:05 - loss: 0.2772 - acc: 0.9062 APL:13.15\n",
            "1280/8000 [===>..........................] - ETA: 3:04 - loss: 0.2824 - acc: 0.9062 APL:13.15\n",
            "1408/8000 [====>.........................] - ETA: 3:01 - loss: 0.2756 - acc: 0.9077 APL:13.87\n",
            "1536/8000 [====>.........................] - ETA: 3:00 - loss: 0.2843 - acc: 0.9043 APL:14.60\n",
            "1664/8000 [=====>........................] - ETA: 2:58 - loss: 0.2857 - acc: 0.9050 APL:13.75\n",
            "1792/8000 [=====>........................] - ETA: 2:55 - loss: 0.2833 - acc: 0.9062 APL:13.72\n",
            "1920/8000 [======>.......................] - ETA: 2:52 - loss: 0.2834 - acc: 0.9062 APL:13.73\n",
            "2048/8000 [======>.......................] - ETA: 2:49 - loss: 0.2861 - acc: 0.9053 APL:14.60\n",
            "2176/8000 [=======>......................] - ETA: 2:46 - loss: 0.2883 - acc: 0.9040 APL:12.60\n",
            "2304/8000 [=======>......................] - ETA: 2:43 - loss: 0.2904 - acc: 0.9032 APL:12.66\n",
            "2432/8000 [========>.....................] - ETA: 2:40 - loss: 0.2875 - acc: 0.9038 APL:12.61\n",
            "2560/8000 [========>.....................] - ETA: 2:37 - loss: 0.2816 - acc: 0.9055 APL:14.62\n",
            "2688/8000 [=========>....................] - ETA: 2:33 - loss: 0.2807 - acc: 0.9055 APL:14.77\n",
            "2816/8000 [=========>....................] - ETA: 2:30 - loss: 0.2812 - acc: 0.9059 APL:14.74\n",
            "2944/8000 [==========>...................] - ETA: 2:27 - loss: 0.2840 - acc: 0.9052 APL:14.60\n",
            "3072/8000 [==========>...................] - ETA: 2:23 - loss: 0.2819 - acc: 0.9066 APL:13.74\n",
            "3200/8000 [===========>..................] - ETA: 2:20 - loss: 0.2862 - acc: 0.9050 APL:12.67\n",
            "3328/8000 [===========>..................] - ETA: 2:16 - loss: 0.2868 - acc: 0.9047 APL:12.78\n",
            "3456/8000 [===========>..................] - ETA: 2:12 - loss: 0.2896 - acc: 0.9031 APL:12.68\n",
            "3584/8000 [============>.................] - ETA: 2:09 - loss: 0.2897 - acc: 0.9035 APL:14.60\n",
            "3712/8000 [============>.................] - ETA: 2:05 - loss: 0.2874 - acc: 0.9044 APL:14.73\n",
            "3840/8000 [=============>................] - ETA: 2:02 - loss: 0.2869 - acc: 0.9039 APL:13.72\n",
            "3968/8000 [=============>................] - ETA: 1:58 - loss: 0.2873 - acc: 0.9035 APL:14.63\n",
            "4096/8000 [==============>...............] - ETA: 1:54 - loss: 0.2878 - acc: 0.9036 APL:12.66\n",
            "4224/8000 [==============>...............] - ETA: 1:51 - loss: 0.2873 - acc: 0.9032 APL:12.68\n",
            "4352/8000 [===============>..............] - ETA: 1:47 - loss: 0.2873 - acc: 0.9023 APL:14.51\n",
            "4480/8000 [===============>..............] - ETA: 1:43 - loss: 0.2882 - acc: 0.9013 APL:12.66\n",
            "4608/8000 [================>.............] - ETA: 1:40 - loss: 0.2885 - acc: 0.9010 APL:14.64\n",
            "4736/8000 [================>.............] - ETA: 1:36 - loss: 0.2877 - acc: 0.9005 APL:14.62\n",
            "4864/8000 [=================>............] - ETA: 1:32 - loss: 0.2892 - acc: 0.8995 APL:14.59\n",
            "4992/8000 [=================>............] - ETA: 1:29 - loss: 0.2893 - acc: 0.9000 APL:14.56\n",
            "5120/8000 [==================>...........] - ETA: 1:25 - loss: 0.2909 - acc: 0.8990 APL:12.97\n",
            "5248/8000 [==================>...........] - ETA: 1:21 - loss: 0.2912 - acc: 0.8992 APL:13.13\n",
            "5376/8000 [===================>..........] - ETA: 1:17 - loss: 0.2933 - acc: 0.8981 APL:13.13\n",
            "5504/8000 [===================>..........] - ETA: 1:13 - loss: 0.2960 - acc: 0.8972 APL:12.97\n",
            "5632/8000 [====================>.........] - ETA: 1:10 - loss: 0.2969 - acc: 0.8965 APL:14.62\n",
            "5760/8000 [====================>.........] - ETA: 1:06 - loss: 0.2963 - acc: 0.8967 APL:13.00\n",
            "5888/8000 [=====================>........] - ETA: 1:02 - loss: 0.2948 - acc: 0.8972 APL:13.13\n",
            "6016/8000 [=====================>........] - ETA: 58s - loss: 0.2971 - acc: 0.8963  APL:12.69\n",
            "6144/8000 [======================>.......] - ETA: 55s - loss: 0.2990 - acc: 0.8952 APL:12.69\n",
            "6272/8000 [======================>.......] - ETA: 51s - loss: 0.2992 - acc: 0.8959 APL:12.69\n",
            "6400/8000 [=======================>......] - ETA: 47s - loss: 0.2992 - acc: 0.8962 APL:14.30\n",
            "6528/8000 [=======================>......] - ETA: 43s - loss: 0.3006 - acc: 0.8952 APL:13.76\n",
            "6656/8000 [=======================>......] - ETA: 39s - loss: 0.2995 - acc: 0.8959 APL:14.56\n",
            "6784/8000 [========================>.....] - ETA: 36s - loss: 0.3000 - acc: 0.8958 APL:14.56\n",
            "6912/8000 [========================>.....] - ETA: 32s - loss: 0.3003 - acc: 0.8955 APL:13.74\n",
            "7040/8000 [=========================>....] - ETA: 28s - loss: 0.3011 - acc: 0.8952 APL:14.26\n",
            "7168/8000 [=========================>....] - ETA: 24s - loss: 0.2997 - acc: 0.8955 APL:14.24\n",
            "7296/8000 [==========================>...] - ETA: 20s - loss: 0.2996 - acc: 0.8951 APL:13.77\n",
            "7424/8000 [==========================>...] - ETA: 17s - loss: 0.2986 - acc: 0.8956 APL:14.60\n",
            "7552/8000 [===========================>..] - ETA: 13s - loss: 0.2977 - acc: 0.8961 APL:14.63\n",
            "7680/8000 [===========================>..] - ETA: 9s - loss: 0.2964 - acc: 0.8969  APL:14.62\n",
            "7808/8000 [============================>.] - ETA: 5s - loss: 0.2953 - acc: 0.8974 APL:14.62\n",
            "7936/8000 [============================>.] - ETA: 1s - loss: 0.2932 - acc: 0.8984 APL:12.69\n",
            " APL:12.86\n",
            "8000/8000 [==============================] - 245s 31ms/step - loss: 0.2928 - acc: 0.8988 - val_loss: 0.2617 - val_acc: 0.9225\n",
            "training surrogate net... [1/3]\n",
            "Epoch 1/20\n",
            "126/126 [==============================] - 4s 31ms/step - loss: 124.4973 - mean_squared_error: 124.4636\n",
            "Epoch 2/20\n",
            "126/126 [==============================] - 1s 8ms/step - loss: 78.0648 - mean_squared_error: 78.0252\n",
            "Epoch 3/20\n",
            "126/126 [==============================] - 1s 8ms/step - loss: 50.1834 - mean_squared_error: 50.1399\n",
            "Epoch 4/20\n",
            "126/126 [==============================] - 1s 8ms/step - loss: 34.6737 - mean_squared_error: 34.6280\n",
            "Epoch 5/20\n",
            "126/126 [==============================] - 1s 8ms/step - loss: 25.1204 - mean_squared_error: 25.0737\n",
            "Epoch 6/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 19.1910 - mean_squared_error: 19.1438\n",
            "Epoch 7/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 15.0254 - mean_squared_error: 14.9779\n",
            "Epoch 8/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 11.9342 - mean_squared_error: 11.8868\n",
            "Epoch 9/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 9.5557 - mean_squared_error: 9.5083\n",
            "Epoch 10/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 7.6353 - mean_squared_error: 7.5880\n",
            "Epoch 11/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 6.0997 - mean_squared_error: 6.0524\n",
            "Epoch 12/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 4.8454 - mean_squared_error: 4.7983\n",
            "Epoch 13/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 3.8824 - mean_squared_error: 3.8354\n",
            "Epoch 14/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 3.0914 - mean_squared_error: 3.0445\n",
            "Epoch 15/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 2.4767 - mean_squared_error: 2.4299\n",
            "Epoch 16/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 2.0084 - mean_squared_error: 1.9617\n",
            "Epoch 17/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 1.6252 - mean_squared_error: 1.5787\n",
            "Epoch 18/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 1.3442 - mean_squared_error: 1.2978\n",
            "Epoch 19/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 1.1204 - mean_squared_error: 1.0742\n",
            "Epoch 20/20\n",
            "126/126 [==============================] - 1s 7ms/step - loss: 0.9666 - mean_squared_error: 0.9205\n",
            "training MLP net... [2/3]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 3s 366us/step - loss: 0.4077 - acc: 0.9018 - val_loss: 0.3303 - val_acc: 0.9404\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 69us/step - loss: 0.3945 - acc: 0.9048 - val_loss: 0.3243 - val_acc: 0.9455\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.3608 - acc: 0.9173 - val_loss: 0.2977 - val_acc: 0.9495\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.3402 - acc: 0.9246 - val_loss: 0.4225 - val_acc: 0.9000\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 67us/step - loss: 0.3277 - acc: 0.9281 - val_loss: 0.3155 - val_acc: 0.9444\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 69us/step - loss: 0.3042 - acc: 0.9376 - val_loss: 0.2993 - val_acc: 0.9465\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 68us/step - loss: 0.2797 - acc: 0.9504 - val_loss: 0.2505 - val_acc: 0.9658\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 68us/step - loss: 0.2641 - acc: 0.9554 - val_loss: 0.2565 - val_acc: 0.9618\n",
            "Epoch 9/10\n",
            " 128/8000 [..............................] - ETA: 0s - loss: 0.2481 - acc: 0.9531 APL:14.67\n",
            " 256/8000 [..............................] - ETA: 2:22 - loss: 0.2542 - acc: 0.9570 APL:12.84\n",
            " 384/8000 [>.............................] - ETA: 2:47 - loss: 0.2438 - acc: 0.9609 APL:13.12\n",
            " 512/8000 [>.............................] - ETA: 2:58 - loss: 0.2431 - acc: 0.9609 APL:14.12\n",
            " 640/8000 [=>............................] - ETA: 3:03 - loss: 0.2439 - acc: 0.9594 APL:12.84\n",
            " 768/8000 [=>............................] - ETA: 3:05 - loss: 0.2378 - acc: 0.9648 APL:14.34\n",
            " 896/8000 [==>...........................] - ETA: 3:07 - loss: 0.2477 - acc: 0.9632 APL:15.05\n",
            "1024/8000 [==>...........................] - ETA: 3:07 - loss: 0.2522 - acc: 0.9600 APL:14.36\n",
            "1152/8000 [===>..........................] - ETA: 3:06 - loss: 0.2560 - acc: 0.9592 APL:14.61\n",
            "1280/8000 [===>..........................] - ETA: 3:05 - loss: 0.2542 - acc: 0.9594 APL:11.89\n",
            "1408/8000 [====>.........................] - ETA: 3:02 - loss: 0.2616 - acc: 0.9567 APL:12.85\n",
            "1536/8000 [====>.........................] - ETA: 2:59 - loss: 0.2623 - acc: 0.9557 APL:12.86\n",
            "1664/8000 [=====>........................] - ETA: 2:56 - loss: 0.2605 - acc: 0.9561 APL:14.68\n",
            "1792/8000 [=====>........................] - ETA: 2:54 - loss: 0.2650 - acc: 0.9542 APL:14.39\n",
            "1920/8000 [======>.......................] - ETA: 2:51 - loss: 0.2673 - acc: 0.9547 APL:14.40\n",
            "2048/8000 [======>.......................] - ETA: 2:48 - loss: 0.2653 - acc: 0.9551 APL:14.98\n",
            "2176/8000 [=======>......................] - ETA: 2:46 - loss: 0.2633 - acc: 0.9563 APL:14.70\n",
            "2304/8000 [=======>......................] - ETA: 2:42 - loss: 0.2628 - acc: 0.9566 APL:12.86\n",
            "2432/8000 [========>.....................] - ETA: 2:39 - loss: 0.2593 - acc: 0.9589 APL:12.85\n",
            "2560/8000 [========>.....................] - ETA: 2:35 - loss: 0.2573 - acc: 0.9594 APL:12.84\n",
            "2688/8000 [=========>....................] - ETA: 2:32 - loss: 0.2565 - acc: 0.9598 APL:12.84\n",
            "2816/8000 [=========>....................] - ETA: 2:28 - loss: 0.2548 - acc: 0.9609 APL:12.84\n",
            "2944/8000 [==========>...................] - ETA: 2:25 - loss: 0.2556 - acc: 0.9606 APL:12.84\n",
            "3072/8000 [==========>...................] - ETA: 2:21 - loss: 0.2556 - acc: 0.9600 APL:14.12\n",
            "3200/8000 [===========>..................] - ETA: 2:17 - loss: 0.2545 - acc: 0.9606 APL:13.36\n",
            "3328/8000 [===========>..................] - ETA: 2:14 - loss: 0.2570 - acc: 0.9588 APL:12.78\n",
            "3456/8000 [===========>..................] - ETA: 2:10 - loss: 0.2560 - acc: 0.9586 APL:14.35\n",
            "3584/8000 [============>.................] - ETA: 2:07 - loss: 0.2579 - acc: 0.9573 APL:14.58\n",
            "3712/8000 [============>.................] - ETA: 2:03 - loss: 0.2584 - acc: 0.9574 APL:14.57\n",
            "3840/8000 [=============>................] - ETA: 2:00 - loss: 0.2584 - acc: 0.9573 APL:14.53\n",
            "3968/8000 [=============>................] - ETA: 1:56 - loss: 0.2604 - acc: 0.9569 APL:13.17\n",
            "4096/8000 [==============>...............] - ETA: 1:52 - loss: 0.2619 - acc: 0.9561 APL:13.21\n",
            "4224/8000 [==============>...............] - ETA: 1:49 - loss: 0.2608 - acc: 0.9567 APL:13.15\n",
            "4352/8000 [===============>..............] - ETA: 1:45 - loss: 0.2600 - acc: 0.9573 APL:12.80\n",
            "4480/8000 [===============>..............] - ETA: 1:42 - loss: 0.2616 - acc: 0.9569 APL:15.11\n",
            "4608/8000 [================>.............] - ETA: 1:38 - loss: 0.2646 - acc: 0.9555 APL:14.51\n",
            "4736/8000 [================>.............] - ETA: 1:34 - loss: 0.2629 - acc: 0.9561 APL:14.57\n",
            "4864/8000 [=================>............] - ETA: 1:31 - loss: 0.2611 - acc: 0.9566 APL:13.16\n",
            "4992/8000 [=================>............] - ETA: 1:27 - loss: 0.2617 - acc: 0.9563 APL:13.16\n",
            "5120/8000 [==================>...........] - ETA: 1:24 - loss: 0.2625 - acc: 0.9564 APL:12.80\n",
            "5248/8000 [==================>...........] - ETA: 1:20 - loss: 0.2635 - acc: 0.9560 APL:12.85\n",
            "5376/8000 [===================>..........] - ETA: 1:16 - loss: 0.2644 - acc: 0.9557 APL:14.63\n",
            "5504/8000 [===================>..........] - ETA: 1:12 - loss: 0.2640 - acc: 0.9560 APL:14.64\n",
            "5632/8000 [====================>.........] - ETA: 1:09 - loss: 0.2638 - acc: 0.9560 APL:14.63\n",
            "5760/8000 [====================>.........] - ETA: 1:05 - loss: 0.2624 - acc: 0.9564 APL:14.61\n",
            "5888/8000 [=====================>........] - ETA: 1:01 - loss: 0.2628 - acc: 0.9564 APL:14.36\n",
            "6016/8000 [=====================>........] - ETA: 58s - loss: 0.2615 - acc: 0.9568  APL:12.79\n",
            "6144/8000 [======================>.......] - ETA: 54s - loss: 0.2608 - acc: 0.9570 APL:14.61\n",
            "6272/8000 [======================>.......] - ETA: 50s - loss: 0.2601 - acc: 0.9573 APL:14.68\n",
            "6400/8000 [=======================>......] - ETA: 46s - loss: 0.2598 - acc: 0.9573 APL:14.56\n",
            "6528/8000 [=======================>......] - ETA: 43s - loss: 0.2603 - acc: 0.9570 APL:14.65\n",
            "6656/8000 [=======================>......] - ETA: 39s - loss: 0.2588 - acc: 0.9576 APL:14.59\n",
            "6784/8000 [========================>.....] - ETA: 35s - loss: 0.2585 - acc: 0.9575 APL:14.65\n",
            "6912/8000 [========================>.....] - ETA: 32s - loss: 0.2589 - acc: 0.9573 APL:14.60\n",
            "7040/8000 [=========================>....] - ETA: 28s - loss: 0.2584 - acc: 0.9575 APL:14.30\n",
            "7168/8000 [=========================>....] - ETA: 24s - loss: 0.2573 - acc: 0.9581 APL:14.11\n",
            "7296/8000 [==========================>...] - ETA: 20s - loss: 0.2583 - acc: 0.9578 APL:14.11\n",
            "7424/8000 [==========================>...] - ETA: 16s - loss: 0.2578 - acc: 0.9578 APL:14.30\n",
            "7552/8000 [===========================>..] - ETA: 13s - loss: 0.2582 - acc: 0.9580 APL:14.62\n",
            "7680/8000 [===========================>..] - ETA: 9s - loss: 0.2585 - acc: 0.9581  APL:14.62\n",
            "7808/8000 [============================>.] - ETA: 5s - loss: 0.2588 - acc: 0.9576 APL:14.60\n",
            "7936/8000 [============================>.] - ETA: 1s - loss: 0.2588 - acc: 0.9577 APL:14.30\n",
            " APL:13.36\n",
            "8000/8000 [==============================] - 242s 30ms/step - loss: 0.2581 - acc: 0.9580 - val_loss: 0.3602 - val_acc: 0.9193\n",
            "Epoch 10/10\n",
            " 128/8000 [..............................] - ETA: 0s - loss: 0.2079 - acc: 0.9922 APL:12.84\n",
            " 256/8000 [..............................] - ETA: 1:51 - loss: 0.2078 - acc: 0.9766 APL:14.14\n",
            " 384/8000 [>.............................] - ETA: 2:28 - loss: 0.2115 - acc: 0.9740 APL:12.61\n",
            " 512/8000 [>.............................] - ETA: 2:43 - loss: 0.2154 - acc: 0.9688 APL:15.07\n",
            " 640/8000 [=>............................] - ETA: 2:54 - loss: 0.2308 - acc: 0.9672 APL:14.59\n",
            " 768/8000 [=>............................] - ETA: 2:59 - loss: 0.2294 - acc: 0.9688 APL:13.25\n",
            " 896/8000 [==>...........................] - ETA: 3:01 - loss: 0.2355 - acc: 0.9665 APL:11.88\n",
            "1024/8000 [==>...........................] - ETA: 3:01 - loss: 0.2420 - acc: 0.9658 APL:12.81\n",
            "1152/8000 [===>..........................] - ETA: 3:00 - loss: 0.2358 - acc: 0.9688 APL:14.62\n",
            "1280/8000 [===>..........................] - ETA: 2:59 - loss: 0.2365 - acc: 0.9688 APL:14.38\n",
            "1408/8000 [====>.........................] - ETA: 2:59 - loss: 0.2399 - acc: 0.9659 APL:14.61\n",
            "1536/8000 [====>.........................] - ETA: 2:58 - loss: 0.2413 - acc: 0.9635 APL:12.78\n",
            "1664/8000 [=====>........................] - ETA: 2:55 - loss: 0.2439 - acc: 0.9627 APL:14.31\n",
            "1792/8000 [=====>........................] - ETA: 2:53 - loss: 0.2435 - acc: 0.9632 APL:14.31\n",
            "1920/8000 [======>.......................] - ETA: 2:50 - loss: 0.2452 - acc: 0.9620 APL:14.25\n",
            "2048/8000 [======>.......................] - ETA: 2:47 - loss: 0.2461 - acc: 0.9614 APL:14.59\n",
            "2176/8000 [=======>......................] - ETA: 2:44 - loss: 0.2465 - acc: 0.9605 APL:14.59\n",
            "2304/8000 [=======>......................] - ETA: 2:41 - loss: 0.2430 - acc: 0.9622 APL:14.59\n",
            "2432/8000 [========>.....................] - ETA: 2:38 - loss: 0.2435 - acc: 0.9622 APL:14.31\n",
            "2560/8000 [========>.....................] - ETA: 2:35 - loss: 0.2400 - acc: 0.9641 APL:13.13\n",
            "2688/8000 [=========>....................] - ETA: 2:32 - loss: 0.2423 - acc: 0.9632 APL:13.13\n",
            "2816/8000 [=========>....................] - ETA: 2:28 - loss: 0.2433 - acc: 0.9631 APL:12.80\n",
            "2944/8000 [==========>...................] - ETA: 2:24 - loss: 0.2420 - acc: 0.9637 APL:14.63\n",
            "3072/8000 [==========>...................] - ETA: 2:21 - loss: 0.2409 - acc: 0.9642 APL:14.55\n",
            "3200/8000 [===========>..................] - ETA: 2:17 - loss: 0.2434 - acc: 0.9625 APL:12.81\n",
            "3328/8000 [===========>..................] - ETA: 2:14 - loss: 0.2442 - acc: 0.9618 APL:12.81\n",
            "3456/8000 [===========>..................] - ETA: 2:10 - loss: 0.2428 - acc: 0.9621 APL:12.79\n",
            "3584/8000 [============>.................] - ETA: 2:07 - loss: 0.2423 - acc: 0.9626 APL:12.79\n",
            "3712/8000 [============>.................] - ETA: 2:03 - loss: 0.2434 - acc: 0.9623 APL:15.07\n",
            "3840/8000 [=============>................] - ETA: 2:00 - loss: 0.2429 - acc: 0.9625 APL:14.66\n",
            "3968/8000 [=============>................] - ETA: 1:56 - loss: 0.2440 - acc: 0.9612 APL:14.61\n",
            "4096/8000 [==============>...............] - ETA: 1:53 - loss: 0.2429 - acc: 0.9617 APL:12.85\n",
            "4224/8000 [==============>...............] - ETA: 1:49 - loss: 0.2441 - acc: 0.9612 APL:12.81\n",
            "4352/8000 [===============>..............] - ETA: 1:45 - loss: 0.2437 - acc: 0.9616 APL:12.86\n",
            "4480/8000 [===============>..............] - ETA: 1:42 - loss: 0.2444 - acc: 0.9612 APL:14.62\n",
            "4608/8000 [================>.............] - ETA: 1:38 - loss: 0.2448 - acc: 0.9612 APL:14.32\n",
            "4736/8000 [================>.............] - ETA: 1:34 - loss: 0.2458 - acc: 0.9609 APL:14.13\n",
            "4864/8000 [=================>............] - ETA: 1:31 - loss: 0.2476 - acc: 0.9595 APL:13.74\n",
            "4992/8000 [=================>............] - ETA: 1:27 - loss: 0.2465 - acc: 0.9603 APL:11.61\n",
            "5120/8000 [==================>...........] - ETA: 1:23 - loss: 0.2491 - acc: 0.9594 APL:13.74\n",
            "5248/8000 [==================>...........] - ETA: 1:20 - loss: 0.2503 - acc: 0.9587 APL:14.58\n",
            "5376/8000 [===================>..........] - ETA: 1:16 - loss: 0.2516 - acc: 0.9581 APL:15.09\n",
            "5504/8000 [===================>..........] - ETA: 1:13 - loss: 0.2573 - acc: 0.9553 APL:14.57\n",
            "5632/8000 [====================>.........] - ETA: 1:09 - loss: 0.2569 - acc: 0.9553 APL:13.26\n",
            "5760/8000 [====================>.........] - ETA: 1:05 - loss: 0.2556 - acc: 0.9559 APL:13.27\n",
            "5888/8000 [=====================>........] - ETA: 1:01 - loss: 0.2576 - acc: 0.9558 APL:13.20\n",
            "6016/8000 [=====================>........] - ETA: 58s - loss: 0.2583 - acc: 0.9553  APL:12.80\n",
            "6144/8000 [======================>.......] - ETA: 54s - loss: 0.2578 - acc: 0.9556 APL:14.89\n",
            "6272/8000 [======================>.......] - ETA: 50s - loss: 0.2595 - acc: 0.9549 APL:14.89\n",
            "6400/8000 [=======================>......] - ETA: 46s - loss: 0.2584 - acc: 0.9553 APL:15.07\n",
            "6528/8000 [=======================>......] - ETA: 43s - loss: 0.2586 - acc: 0.9553 APL:14.61\n",
            "6656/8000 [=======================>......] - ETA: 39s - loss: 0.2604 - acc: 0.9548 APL:13.23\n",
            "6784/8000 [========================>.....] - ETA: 35s - loss: 0.2614 - acc: 0.9543 APL:13.20\n",
            "6912/8000 [========================>.....] - ETA: 32s - loss: 0.2620 - acc: 0.9541 APL:13.21\n",
            "7040/8000 [=========================>....] - ETA: 28s - loss: 0.2618 - acc: 0.9541 APL:12.87\n",
            "7168/8000 [=========================>....] - ETA: 24s - loss: 0.2617 - acc: 0.9542 APL:14.59\n",
            "7296/8000 [==========================>...] - ETA: 20s - loss: 0.2620 - acc: 0.9544 APL:15.12\n",
            "7424/8000 [==========================>...] - ETA: 16s - loss: 0.2630 - acc: 0.9543 APL:14.49\n",
            "7552/8000 [===========================>..] - ETA: 13s - loss: 0.2624 - acc: 0.9546 APL:11.92\n",
            "7680/8000 [===========================>..] - ETA: 9s - loss: 0.2619 - acc: 0.9549  APL:13.44\n",
            "7808/8000 [============================>.] - ETA: 5s - loss: 0.2614 - acc: 0.9552 APL:13.50\n",
            "7936/8000 [============================>.] - ETA: 1s - loss: 0.2617 - acc: 0.9548 APL:12.80\n",
            " APL:15.07\n",
            "8000/8000 [==============================] - 241s 30ms/step - loss: 0.2621 - acc: 0.9543 - val_loss: 0.2483 - val_acc: 0.9615\n",
            "training surrogate net... [2/3]\n",
            "Epoch 1/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.8395 - mean_squared_error: 0.7936\n",
            "Epoch 2/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7746 - mean_squared_error: 0.7289\n",
            "Epoch 3/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7376 - mean_squared_error: 0.6922\n",
            "Epoch 4/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7185 - mean_squared_error: 0.6734\n",
            "Epoch 5/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7125 - mean_squared_error: 0.6676\n",
            "Epoch 6/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7125 - mean_squared_error: 0.6679\n",
            "Epoch 7/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7116 - mean_squared_error: 0.6673\n",
            "Epoch 8/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7075 - mean_squared_error: 0.6634\n",
            "Epoch 9/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7099 - mean_squared_error: 0.6661\n",
            "Epoch 10/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7066 - mean_squared_error: 0.6630\n",
            "Epoch 11/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7083 - mean_squared_error: 0.6649\n",
            "Epoch 12/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7082 - mean_squared_error: 0.6652\n",
            "Epoch 13/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7066 - mean_squared_error: 0.6638\n",
            "Epoch 14/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7113 - mean_squared_error: 0.6688\n",
            "Epoch 15/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7096 - mean_squared_error: 0.6673\n",
            "Epoch 16/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7078 - mean_squared_error: 0.6657\n",
            "Epoch 17/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7040 - mean_squared_error: 0.6622\n",
            "Epoch 18/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7021 - mean_squared_error: 0.6604\n",
            "Epoch 19/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7070 - mean_squared_error: 0.6656\n",
            "Epoch 20/20\n",
            "252/252 [==============================] - 2s 7ms/step - loss: 0.7022 - mean_squared_error: 0.6611\n",
            "training MLP net... [3/3]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 3s 397us/step - loss: 0.2551 - acc: 0.9580 - val_loss: 0.2611 - val_acc: 0.9596\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.2398 - acc: 0.9651 - val_loss: 0.3078 - val_acc: 0.9444\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.2283 - acc: 0.9691 - val_loss: 0.3237 - val_acc: 0.9371\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 72us/step - loss: 0.2267 - acc: 0.9695 - val_loss: 0.2483 - val_acc: 0.9658\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.2183 - acc: 0.9739 - val_loss: 0.2876 - val_acc: 0.9495\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.2381 - acc: 0.9645 - val_loss: 0.2174 - val_acc: 0.9753\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.2176 - acc: 0.9738 - val_loss: 0.3166 - val_acc: 0.9411\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 75us/step - loss: 0.2199 - acc: 0.9714 - val_loss: 0.2098 - val_acc: 0.9756\n",
            "Epoch 9/10\n",
            " 128/8000 [..............................] - ETA: 0s - loss: 0.2619 - acc: 0.9609 APL:15.22\n",
            " 256/8000 [..............................] - ETA: 2:25 - loss: 0.3174 - acc: 0.9375 APL:14.91\n",
            " 384/8000 [>.............................] - ETA: 2:54 - loss: 0.2919 - acc: 0.9453 APL:15.11\n",
            " 512/8000 [>.............................] - ETA: 3:07 - loss: 0.2813 - acc: 0.9453 APL:12.49\n",
            " 640/8000 [=>............................] - ETA: 3:12 - loss: 0.2739 - acc: 0.9531"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-b6412987afb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters_retrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_sur\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_mlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-b365a86d8398>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, iters_retrain, epochs_mlp, epochs_sur, batch_size, feature_names, validation_data, class_names)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training MLP net... [%d/%d]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters_retrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training surrogate net... [%d/%d]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters_retrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_apl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_sur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-b365a86d8398>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, batch_size, epochs, validation_data)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAplHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-b365a86d8398>\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                     \u001b[0mapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_apl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-b365a86d8398>\u001b[0m in \u001b[0;36maverage_path_length\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maverage_path_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mpath_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m#Compute average path length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-b365a86d8398>\u001b[0m in \u001b[0;36mfit_tree\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m\"\"\"Train decision tree to track path length.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0my_train_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;31m# y_train_hat_int = np.rint(y_train_hat).astype(int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train_hat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "yoec0xQfnhxj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Draw the figure"
      ]
    },
    {
      "metadata": {
        "id": "WWHhudo3obRs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_points(path):\n",
        "    try:\n",
        "        f = open(path + 'log.txt', 'rb')\n",
        "    except:\n",
        "        return [], []\n",
        "    fs = f.readlines()\n",
        "    APL = []\n",
        "    acc = []\n",
        "    for l in fs:\n",
        "        l = l.decode(\"utf-8\").strip()\n",
        "        if float(l.split(' ')[-1]) > 60:\n",
        "          continue\n",
        "        APL.append(float(l.split(' ')[-1]))\n",
        "        acc.append(float(l.split(';')[0].split(' ')[-1]))\n",
        "        \n",
        "    APL, acc = np.array(APL)[np.argsort(APL)], np.array(acc)[np.argsort(APL)]\n",
        "    \n",
        "    return APL, acc\n",
        "  \n",
        "def drawtest(t=''):\n",
        "    path_n = t + 'tree_n/'\n",
        "    path_t = t + 'tree_t/'\n",
        "    path_l2 = t + 'tree_l2/'\n",
        "    path_l1 = t + 'tree_l1/'\n",
        "    apl_n, acc_n = get_points(path_n)\n",
        "    apl_t, acc_t = get_points(path_t)\n",
        "    apl_l2, acc_l2 = get_points(path_l2)\n",
        "    apl_l1, acc_l1 = get_points(path_l1)\n",
        "    %matplotlib inline\n",
        "    fig = plt.figure(figsize =(8, 5), dpi=100, facecolor=\"white\")\n",
        "    font={'family': 'serif', 'color':'darkred', 'weight':'normal', 'size':16} \n",
        "    ax = plt.gca()\n",
        "    plt.plot(apl_n, acc_n, c='b') \n",
        "    plt.plot(apl_t, acc_t, c='y') \n",
        "    plt.plot(apl_l2, acc_l2, c='r') \n",
        "    plt.plot(apl_l1, acc_l1, c='g') \n",
        "    plt.legend([\"Decision Tree\", \"Tree Regularization\", \"l2 Regularization\", \"l1 Regularization\"]) \n",
        "    plt.xlabel(\"APL\", font)\n",
        "    plt.ylabel(\"Accuracy\", font)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XNtv2e-4olY3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Get samples from tree regularization network"
      ]
    },
    {
      "metadata": {
        "id": "DXghy9oqph9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "9238935f-5aee-42be-a5e0-0ae1b0119052"
      },
      "cell_type": "code",
      "source": [
        "name = 't'\n",
        "# y_train_hat\n",
        "y_train_hat = t.predict(X)\n",
        "y_pred = [np.argmax(x) for x in y_train_hat]\n",
        "l = [1,2,3,4,5,6,7,8,9,10] + [12, 14, 16, 18, 20] + [25, 30, 40, 50, 80, 120, 160, 200, 250, 300, 350, 400, 500, 800, 1000, 2000, 2500, 3000]\n",
        "for i in range(len(l)):\n",
        "  tree = DecisionTreeClassifier(min_samples_leaf=l[i])\n",
        "  tree.fit(X, y_pred)\n",
        "  if not os.path.isdir('./tree_'+name):\n",
        "    os.mkdir('./tree_'+name)\n",
        "  nodes = visualize(tree, './tree_'+name+'/tree' + str(i) + '.pdf',False,list(df.columns.values), class_names=list(cat_index.keys()))\n",
        "  acc = accuracy_score(tree.predict(X_test), [np.argmax(x) for x in Y_test])\n",
        "  leaf_indices = tree.apply(X)\n",
        "  leaf_counts = np.bincount(leaf_indices)\n",
        "  leaf_i = np.arange(tree.tree_.node_count)\n",
        "  apl = np.dot(leaf_i, leaf_counts) / float(X.shape[0])\n",
        "  if APL_mode == 1:\n",
        "    apl = np.sum(tree.decision_path(X)) / float(X.shape[0])\n",
        "  print('tree'+ str(i) + ': accuracy {:.5f}; number of nodes '.format(acc) + str(nodes) + '; APL {:.2f}'.format(apl))\n",
        "  log = open('./tree_'+name+'/log.txt', 'a')\n",
        "  log.write('tree'+ str(i) + ': accuracy {:.5f}; number of nodes '.format(acc) + str(nodes) + '; APL {:.2f}\\n'.format(apl))\n",
        "  log.close()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tree0: accuracy 0.84036; number of nodes 554; APL 301.75\n",
            "tree1: accuracy 0.82873; number of nodes 486; APL 270.13\n",
            "tree2: accuracy 0.83745; number of nodes 450; APL 250.90\n",
            "tree3: accuracy 0.83782; number of nodes 392; APL 220.27\n",
            "tree4: accuracy 0.84182; number of nodes 364; APL 205.36\n",
            "tree5: accuracy 0.84109; number of nodes 330; APL 184.90\n",
            "tree6: accuracy 0.84400; number of nodes 314; APL 173.97\n",
            "tree7: accuracy 0.84400; number of nodes 296; APL 164.96\n",
            "tree8: accuracy 0.84800; number of nodes 292; APL 163.42\n",
            "tree9: accuracy 0.83782; number of nodes 288; APL 160.23\n",
            "tree10: accuracy 0.84327; number of nodes 262; APL 146.44\n",
            "tree11: accuracy 0.84400; number of nodes 236; APL 127.63\n",
            "tree12: accuracy 0.85636; number of nodes 222; APL 121.18\n",
            "tree13: accuracy 0.84909; number of nodes 206; APL 111.30\n",
            "tree14: accuracy 0.84945; number of nodes 202; APL 108.51\n",
            "tree15: accuracy 0.85564; number of nodes 178; APL 97.20\n",
            "tree16: accuracy 0.85964; number of nodes 164; APL 89.49\n",
            "tree17: accuracy 0.84545; number of nodes 138; APL 74.81\n",
            "tree18: accuracy 0.82982; number of nodes 116; APL 65.69\n",
            "tree19: accuracy 0.84327; number of nodes 92; APL 49.50\n",
            "tree20: accuracy 0.81673; number of nodes 70; APL 37.24\n",
            "tree21: accuracy 0.86036; number of nodes 56; APL 27.83\n",
            "tree22: accuracy 0.84364; number of nodes 50; APL 26.22\n",
            "tree23: accuracy 0.86618; number of nodes 44; APL 22.65\n",
            "tree24: accuracy 0.79455; number of nodes 38; APL 18.83\n",
            "tree25: accuracy 0.77927; number of nodes 32; APL 17.21\n",
            "tree26: accuracy 0.76655; number of nodes 30; APL 16.47\n",
            "tree27: accuracy 0.85091; number of nodes 24; APL 12.31\n",
            "tree28: accuracy 0.85018; number of nodes 16; APL 8.48\n",
            "tree29: accuracy 0.82618; number of nodes 14; APL 7.73\n",
            "tree30: accuracy 0.58145; number of nodes 6; APL 2.55\n",
            "tree31: accuracy 0.73673; number of nodes 4; APL 1.60\n",
            "tree32: accuracy 0.73673; number of nodes 4; APL 1.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bUh09B-4evW9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get samples from Decision Tree"
      ]
    },
    {
      "metadata": {
        "id": "t7vZSm2BVD6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1700
        },
        "outputId": "d09fd4f2-e8c5-45c7-96ee-46fdd5a2baa0"
      },
      "cell_type": "code",
      "source": [
        "name = 'n'\n",
        "#y_train_hat\n",
        "y_pred = [np.argmax(x) for x in Y]\n",
        "l = [1,2,3,4,5,6,7,8,9,10] + [12, 14, 16, 18, 20] + [25, 30, 40, 50, 80, 120, 160, 200, 250, 300, 350, 400, 500, 800, 1000, 2000, 2500, 3000]\n",
        "for i in range(len(l)):\n",
        "  tree = DecisionTreeClassifier(min_samples_leaf=l[i])\n",
        "  tree.fit(X, y_pred)\n",
        "  if not os.path.isdir('./tree_'+name):\n",
        "    os.mkdir('./tree_'+name)\n",
        "  nodes = visualize(tree, './tree_'+name+'/tree' + str(i) + '.pdf',False,list(df.columns.values), class_names=list(cat_index.keys()))\n",
        "  acc = accuracy_score(tree.predict(X_test), [np.argmax(x) for x in Y_test])\n",
        "  leaf_indices = tree.apply(X)\n",
        "  leaf_counts = np.bincount(leaf_indices)\n",
        "  leaf_i = np.arange(tree.tree_.node_count)\n",
        "  apl = np.dot(leaf_i, leaf_counts) / float(X.shape[0])\n",
        "  if APL_mode == 1:\n",
        "    apl = np.sum(tree.decision_path(X)) / float(X.shape[0])\n",
        "  print('tree'+ str(i) + ': accuracy {:.5f}; number of nodes '.format(acc) + str(nodes) + '; APL {:.2f}\\n'.format(apl))\n",
        "  log = open('./tree_'+name+'/log.txt', 'a')\n",
        "  log.write('tree'+ str(i) + ': accuracy {:.5f}; number of nodes '.format(acc) + str(nodes) + '; APL {:.2f}\\n'.format(apl))\n",
        "  log.close()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of nodes: 198\n",
            "tree0: accuracy 0.98036; number of nodes 198; APL 121.41\n",
            "\n",
            "number of nodes: 188\n",
            "tree1: accuracy 0.97927; number of nodes 188; APL 115.87\n",
            "\n",
            "number of nodes: 178\n",
            "tree2: accuracy 0.98218; number of nodes 178; APL 109.87\n",
            "\n",
            "number of nodes: 166\n",
            "tree3: accuracy 0.98400; number of nodes 166; APL 103.83\n",
            "\n",
            "number of nodes: 158\n",
            "tree4: accuracy 0.98400; number of nodes 158; APL 98.65\n",
            "\n",
            "number of nodes: 154\n",
            "tree5: accuracy 0.98473; number of nodes 154; APL 96.38\n",
            "\n",
            "number of nodes: 154\n",
            "tree6: accuracy 0.98473; number of nodes 154; APL 97.03\n",
            "\n",
            "number of nodes: 154\n",
            "tree7: accuracy 0.98109; number of nodes 154; APL 95.28\n",
            "\n",
            "number of nodes: 144\n",
            "tree8: accuracy 0.98145; number of nodes 144; APL 90.43\n",
            "\n",
            "number of nodes: 144\n",
            "tree9: accuracy 0.98182; number of nodes 144; APL 90.42\n",
            "\n",
            "number of nodes: 138\n",
            "tree10: accuracy 0.97782; number of nodes 138; APL 85.70\n",
            "\n",
            "number of nodes: 134\n",
            "tree11: accuracy 0.97527; number of nodes 134; APL 83.21\n",
            "\n",
            "number of nodes: 134\n",
            "tree12: accuracy 0.97309; number of nodes 134; APL 83.19\n",
            "\n",
            "number of nodes: 130\n",
            "tree13: accuracy 0.97455; number of nodes 130; APL 80.22\n",
            "\n",
            "number of nodes: 122\n",
            "tree14: accuracy 0.97527; number of nodes 122; APL 75.25\n",
            "\n",
            "number of nodes: 108\n",
            "tree15: accuracy 0.97200; number of nodes 108; APL 67.17\n",
            "\n",
            "number of nodes: 98\n",
            "tree16: accuracy 0.96509; number of nodes 98; APL 58.71\n",
            "\n",
            "number of nodes: 88\n",
            "tree17: accuracy 0.97382; number of nodes 88; APL 52.16\n",
            "\n",
            "number of nodes: 78\n",
            "tree18: accuracy 0.96473; number of nodes 78; APL 47.69\n",
            "\n",
            "number of nodes: 68\n",
            "tree19: accuracy 0.95236; number of nodes 68; APL 39.54\n",
            "\n",
            "number of nodes: 58\n",
            "tree20: accuracy 0.94109; number of nodes 58; APL 33.71\n",
            "\n",
            "number of nodes: 50\n",
            "tree21: accuracy 0.94764; number of nodes 50; APL 27.77\n",
            "\n",
            "number of nodes: 46\n",
            "tree22: accuracy 0.93273; number of nodes 46; APL 25.60\n",
            "\n",
            "number of nodes: 40\n",
            "tree23: accuracy 0.92327; number of nodes 40; APL 21.74\n",
            "\n",
            "number of nodes: 36\n",
            "tree24: accuracy 0.91018; number of nodes 36; APL 19.98\n",
            "\n",
            "number of nodes: 32\n",
            "tree25: accuracy 0.88509; number of nodes 32; APL 17.21\n",
            "\n",
            "number of nodes: 30\n",
            "tree26: accuracy 0.91055; number of nodes 30; APL 16.78\n",
            "\n",
            "number of nodes: 28\n",
            "tree27: accuracy 0.85091; number of nodes 28; APL 15.09\n",
            "\n",
            "number of nodes: 18\n",
            "tree28: accuracy 0.88327; number of nodes 18; APL 9.56\n",
            "\n",
            "number of nodes: 10\n",
            "tree29: accuracy 0.78073; number of nodes 10; APL 5.08\n",
            "\n",
            "number of nodes: 6\n",
            "tree30: accuracy 0.84327; number of nodes 6; APL 2.50\n",
            "\n",
            "number of nodes: 4\n",
            "tree31: accuracy 0.59709; number of nodes 4; APL 1.58\n",
            "\n",
            "number of nodes: 4\n",
            "tree32: accuracy 0.59709; number of nodes 4; APL 1.58\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_gRFq06yezeV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get samples from l1, l2 regularization network"
      ]
    },
    {
      "metadata": {
        "id": "0fGM1T1ImHJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tree_regularization import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kuHeFSwYmLsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1159
        },
        "outputId": "7b57961d-8453-4569-e48f-be7f2030e0fd"
      },
      "cell_type": "code",
      "source": [
        "t_l1 = L1MLP(X.shape[1], len(cat), [20, 50, 50, 50], strength=0.01, APL_mode=APL_mode)\n",
        "t_l1.train(X, Y, batch_size=128, iters_retrain=5, epochs_sur=20, epochs_mlp=10, feature_names=list(df.columns.values), validation_data=(X_test, Y_test), class_names=list(cat_index.keys()))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training MLP net... [1/3]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 2s 236us/step - loss: 0.7102 - acc: 0.4950 - val_loss: 0.7519 - val_acc: 0.0909\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 72us/step - loss: 0.6990 - acc: 0.5457 - val_loss: 0.7101 - val_acc: 0.4218\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.5955 - acc: 0.7439 - val_loss: 0.4030 - val_acc: 0.8804\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.4280 - acc: 0.8383 - val_loss: 0.3298 - val_acc: 0.9131\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 72us/step - loss: 0.4080 - acc: 0.8498 - val_loss: 0.3803 - val_acc: 0.8807\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.3767 - acc: 0.8695 - val_loss: 0.3667 - val_acc: 0.8956\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.3564 - acc: 0.8825 - val_loss: 0.3900 - val_acc: 0.8869\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.3403 - acc: 0.8906 - val_loss: 0.2837 - val_acc: 0.9236\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.3474 - acc: 0.8832 - val_loss: 0.3203 - val_acc: 0.9095\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.3239 - acc: 0.8934 - val_loss: 0.3784 - val_acc: 0.8895\n",
            "training MLP net... [2/3]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 2s 258us/step - loss: 0.3019 - acc: 0.9048 - val_loss: 0.2243 - val_acc: 0.9400\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.2937 - acc: 0.9026 - val_loss: 0.3012 - val_acc: 0.9156\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.2617 - acc: 0.9149 - val_loss: 0.2223 - val_acc: 0.9393\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 76us/step - loss: 0.2565 - acc: 0.9160 - val_loss: 0.2239 - val_acc: 0.9378\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.2218 - acc: 0.9290 - val_loss: 0.2620 - val_acc: 0.9178\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.2003 - acc: 0.9389 - val_loss: 0.2333 - val_acc: 0.9295\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.1956 - acc: 0.9401 - val_loss: 0.3118 - val_acc: 0.8971\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.1902 - acc: 0.9430 - val_loss: 0.1690 - val_acc: 0.9520\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.1689 - acc: 0.9486 - val_loss: 0.1776 - val_acc: 0.9505\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.1526 - acc: 0.9573 - val_loss: 0.1428 - val_acc: 0.9585\n",
            "training MLP net... [3/3]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 2s 263us/step - loss: 0.1626 - acc: 0.9523 - val_loss: 0.1450 - val_acc: 0.9604\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.1425 - acc: 0.9586 - val_loss: 0.1782 - val_acc: 0.9476\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.1318 - acc: 0.9644 - val_loss: 0.1750 - val_acc: 0.9491\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.1341 - acc: 0.9615 - val_loss: 0.1903 - val_acc: 0.9404\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.1247 - acc: 0.9675 - val_loss: 0.1379 - val_acc: 0.9611\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.1305 - acc: 0.9609 - val_loss: 0.1230 - val_acc: 0.9676\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.1080 - acc: 0.9732 - val_loss: 0.1998 - val_acc: 0.9353\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 72us/step - loss: 0.1151 - acc: 0.9691 - val_loss: 0.1043 - val_acc: 0.9713\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.1272 - acc: 0.9616 - val_loss: 0.1678 - val_acc: 0.9498\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 1s 74us/step - loss: 0.1059 - acc: 0.9738 - val_loss: 0.1525 - val_acc: 0.9549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YYgN9B03p4yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1907
        },
        "outputId": "272eeff8-343c-4a8c-8275-77eab61db623"
      },
      "cell_type": "code",
      "source": [
        "t_l2 = L2MLP(X.shape[1], len(cat), [20, 50, 50, 50], strength=0.01, APL_mode=APL_mode)\n",
        "t_l2.train(X, Y, batch_size=128, iters_retrain=5, epochs_sur=20, epochs_mlp=10, feature_names=list(df.columns.values), validation_data=(X_test, Y_test), class_names=list(cat_index.keys()))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training MLP net... [1/5]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 2s 302us/step - loss: 0.6828 - acc: 0.5817 - val_loss: 0.6680 - val_acc: 0.6985\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.5585 - acc: 0.7478 - val_loss: 0.4497 - val_acc: 0.8298\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.4232 - acc: 0.8327 - val_loss: 0.4111 - val_acc: 0.8509\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.3846 - acc: 0.8526 - val_loss: 0.4643 - val_acc: 0.8269\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.3572 - acc: 0.8720 - val_loss: 0.4099 - val_acc: 0.8647\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.3506 - acc: 0.8722 - val_loss: 0.3338 - val_acc: 0.8967\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.3303 - acc: 0.8835 - val_loss: 0.3690 - val_acc: 0.8811\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.3085 - acc: 0.8982 - val_loss: 0.3238 - val_acc: 0.8993\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.2910 - acc: 0.9012 - val_loss: 0.3474 - val_acc: 0.8858\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.2733 - acc: 0.9039 - val_loss: 0.2440 - val_acc: 0.9218\n",
            "training MLP net... [2/5]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 2s 299us/step - loss: 0.2659 - acc: 0.9048 - val_loss: 0.3113 - val_acc: 0.8953\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 67us/step - loss: 0.2343 - acc: 0.9163 - val_loss: 0.2199 - val_acc: 0.9291\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.2194 - acc: 0.9243 - val_loss: 0.2072 - val_acc: 0.9298\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.1976 - acc: 0.9293 - val_loss: 0.2849 - val_acc: 0.8945\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 72us/step - loss: 0.1779 - acc: 0.9377 - val_loss: 0.2184 - val_acc: 0.9215\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 72us/step - loss: 0.1700 - acc: 0.9395 - val_loss: 0.1169 - val_acc: 0.9640\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 71us/step - loss: 0.1603 - acc: 0.9440 - val_loss: 0.2371 - val_acc: 0.9124\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 68us/step - loss: 0.1421 - acc: 0.9500 - val_loss: 0.1381 - val_acc: 0.9527\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 1s 68us/step - loss: 0.1267 - acc: 0.9559 - val_loss: 0.1505 - val_acc: 0.9505\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 1s 69us/step - loss: 0.1255 - acc: 0.9555 - val_loss: 0.1549 - val_acc: 0.9469\n",
            "training MLP net... [3/5]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.1220 - acc: 0.9570 - val_loss: 0.1195 - val_acc: 0.9596\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 67us/step - loss: 0.1036 - acc: 0.9631 - val_loss: 0.0902 - val_acc: 0.9727\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 69us/step - loss: 0.1005 - acc: 0.9634 - val_loss: 0.2400 - val_acc: 0.9029\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 68us/step - loss: 0.1110 - acc: 0.9589 - val_loss: 0.2253 - val_acc: 0.9160\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.0953 - acc: 0.9647 - val_loss: 0.1226 - val_acc: 0.9611\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 68us/step - loss: 0.0817 - acc: 0.9734 - val_loss: 0.0804 - val_acc: 0.9749\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 67us/step - loss: 0.0927 - acc: 0.9646 - val_loss: 0.1830 - val_acc: 0.9360\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 65us/step - loss: 0.0905 - acc: 0.9666 - val_loss: 0.1721 - val_acc: 0.9393\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 1s 67us/step - loss: 0.0691 - acc: 0.9762 - val_loss: 0.1603 - val_acc: 0.9433\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 1s 67us/step - loss: 0.0678 - acc: 0.9776 - val_loss: 0.1129 - val_acc: 0.9640\n",
            "training MLP net... [4/5]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 0.1015 - acc: 0.9637 - val_loss: 0.1273 - val_acc: 0.9578\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 69us/step - loss: 0.0611 - acc: 0.9799 - val_loss: 0.1150 - val_acc: 0.9640\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.0579 - acc: 0.9811 - val_loss: 0.2267 - val_acc: 0.9182\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 75us/step - loss: 0.0591 - acc: 0.9809 - val_loss: 0.1040 - val_acc: 0.9684\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 78us/step - loss: 0.0541 - acc: 0.9832 - val_loss: 0.1834 - val_acc: 0.9382\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 78us/step - loss: 0.0526 - acc: 0.9828 - val_loss: 0.1834 - val_acc: 0.9371\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 80us/step - loss: 0.0515 - acc: 0.9834 - val_loss: 0.0828 - val_acc: 0.9745\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 79us/step - loss: 0.0539 - acc: 0.9809 - val_loss: 0.1205 - val_acc: 0.9673\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 1s 78us/step - loss: 0.0886 - acc: 0.9661 - val_loss: 0.1121 - val_acc: 0.9655\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 1s 79us/step - loss: 0.0455 - acc: 0.9873 - val_loss: 0.0832 - val_acc: 0.9756\n",
            "training MLP net... [5/5]\n",
            "Train on 8000 samples, validate on 2750 samples\n",
            "Epoch 1/10\n",
            "8000/8000 [==============================] - 3s 343us/step - loss: 0.0534 - acc: 0.9820 - val_loss: 0.1027 - val_acc: 0.9705\n",
            "Epoch 2/10\n",
            "8000/8000 [==============================] - 1s 75us/step - loss: 0.0464 - acc: 0.9852 - val_loss: 0.1056 - val_acc: 0.9691\n",
            "Epoch 3/10\n",
            "8000/8000 [==============================] - 1s 78us/step - loss: 0.0497 - acc: 0.9821 - val_loss: 0.0829 - val_acc: 0.9767\n",
            "Epoch 4/10\n",
            "8000/8000 [==============================] - 1s 76us/step - loss: 0.0445 - acc: 0.9869 - val_loss: 0.0798 - val_acc: 0.9767\n",
            "Epoch 5/10\n",
            "8000/8000 [==============================] - 1s 75us/step - loss: 0.0493 - acc: 0.9832 - val_loss: 0.0846 - val_acc: 0.9764\n",
            "Epoch 6/10\n",
            "8000/8000 [==============================] - 1s 75us/step - loss: 0.0448 - acc: 0.9848 - val_loss: 0.0927 - val_acc: 0.9720\n",
            "Epoch 7/10\n",
            "8000/8000 [==============================] - 1s 72us/step - loss: 0.0401 - acc: 0.9868 - val_loss: 0.0801 - val_acc: 0.9785\n",
            "Epoch 8/10\n",
            "8000/8000 [==============================] - 1s 70us/step - loss: 0.0425 - acc: 0.9865 - val_loss: 0.0756 - val_acc: 0.9793\n",
            "Epoch 9/10\n",
            "8000/8000 [==============================] - 1s 73us/step - loss: 0.0477 - acc: 0.9836 - val_loss: 0.1086 - val_acc: 0.9662\n",
            "Epoch 10/10\n",
            "8000/8000 [==============================] - 1s 72us/step - loss: 0.0403 - acc: 0.9874 - val_loss: 0.2152 - val_acc: 0.9327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7fJQ_YvhoheX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "ea59a8c7-9388-48da-cd74-aaf129ff5996"
      },
      "cell_type": "code",
      "source": [
        "name = 'l1'\n",
        "# y_train_hat\n",
        "y_train_hat = t_l1.predict(X)\n",
        "y_pred = [np.argmax(x) for x in y_train_hat]\n",
        "l = [1,2,3,4,5,6,7,8,9,10] + [12, 14, 16, 18, 20] + [25, 30, 40, 50, 80, 120, 160, 200, 250, 300, 350, 400, 500, 800, 1000, 2000, 2500, 3000]\n",
        "for i in range(len(l)):\n",
        "  tree = DecisionTreeClassifier(min_samples_leaf=l[i])\n",
        "  tree.fit(X, y_pred)\n",
        "  if not os.path.isdir('./tree_'+name):\n",
        "    os.mkdir('./tree_'+name)\n",
        "  nodes = visualize(tree, './tree_'+name+'/tree' + str(i) + '.pdf',False,list(df.columns.values), class_names=list(cat_index.keys()))\n",
        "  acc = accuracy_score(tree.predict(X_test), [np.argmax(x) for x in Y_test])\n",
        "  leaf_indices = tree.apply(X)\n",
        "  leaf_counts = np.bincount(leaf_indices)\n",
        "  leaf_i = np.arange(tree.tree_.node_count)\n",
        "  apl = np.dot(leaf_i, leaf_counts) / float(X.shape[0])\n",
        "  if APL_mode == 1:\n",
        "    apl = np.sum(tree.decision_path(X)) / float(X.shape[0])\n",
        "  print('tree'+ str(i) + ': accuracy {:.5f}; number of nodes '.format(acc) + str(nodes) + '; APL {:.2f}'.format(apl))\n",
        "  log = open('./tree_'+name+'/log.txt', 'a')\n",
        "  log.write('tree'+ str(i) + ': accuracy {:.5f}; number of nodes '.format(acc) + str(nodes) + '; APL {:.2f}\\n'.format(apl))\n",
        "  log.close()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tree0: accuracy 0.96836; number of nodes 262; APL 165.34\n",
            "tree1: accuracy 0.96545; number of nodes 248; APL 161.45\n",
            "tree2: accuracy 0.96800; number of nodes 232; APL 150.30\n",
            "tree3: accuracy 0.96473; number of nodes 224; APL 145.80\n",
            "tree4: accuracy 0.96945; number of nodes 202; APL 131.74\n",
            "tree5: accuracy 0.96618; number of nodes 192; APL 124.72\n",
            "tree6: accuracy 0.96509; number of nodes 186; APL 121.18\n",
            "tree7: accuracy 0.96873; number of nodes 172; APL 113.55\n",
            "tree8: accuracy 0.96655; number of nodes 166; APL 109.34\n",
            "tree9: accuracy 0.96655; number of nodes 166; APL 109.14\n",
            "tree10: accuracy 0.96000; number of nodes 162; APL 105.92\n",
            "tree11: accuracy 0.95964; number of nodes 156; APL 102.55\n",
            "tree12: accuracy 0.96655; number of nodes 140; APL 91.08\n",
            "tree13: accuracy 0.96291; number of nodes 128; APL 82.90\n",
            "tree14: accuracy 0.95418; number of nodes 124; APL 79.42\n",
            "tree15: accuracy 0.95345; number of nodes 116; APL 74.60\n",
            "tree16: accuracy 0.94909; number of nodes 108; APL 68.24\n",
            "tree17: accuracy 0.95418; number of nodes 96; APL 60.77\n",
            "tree18: accuracy 0.96400; number of nodes 88; APL 54.22\n",
            "tree19: accuracy 0.93418; number of nodes 68; APL 40.45\n",
            "tree20: accuracy 0.93382; number of nodes 58; APL 33.89\n",
            "tree21: accuracy 0.93382; number of nodes 52; APL 29.08\n",
            "tree22: accuracy 0.88982; number of nodes 46; APL 26.45\n",
            "tree23: accuracy 0.87273; number of nodes 38; APL 20.80\n",
            "tree24: accuracy 0.85782; number of nodes 38; APL 20.71\n",
            "tree25: accuracy 0.86145; number of nodes 32; APL 17.04\n",
            "tree26: accuracy 0.84691; number of nodes 30; APL 16.71\n",
            "tree27: accuracy 0.82836; number of nodes 28; APL 14.72\n",
            "tree28: accuracy 0.93309; number of nodes 16; APL 8.54\n",
            "tree29: accuracy 0.90545; number of nodes 12; APL 5.80\n",
            "tree30: accuracy 0.81673; number of nodes 6; APL 2.67\n",
            "tree31: accuracy 0.84073; number of nodes 6; APL 2.63\n",
            "tree32: accuracy 0.59709; number of nodes 4; APL 1.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pGLwZCJ6qT3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "0d0cd85c-42a4-4d10-ea63-284cd21d6d58"
      },
      "cell_type": "code",
      "source": [
        "name = 'l2'\n",
        "# y_train_hat\n",
        "y_train_hat = t_l2.predict(X)\n",
        "y_pred = [np.argmax(x) for x in y_train_hat]\n",
        "l = [1,2,3,4,5,6,7,8,9,10] + [12, 14, 16, 18, 20] + [25, 30, 40, 50, 80, 120, 160, 200, 250, 300, 350, 400, 500, 800, 1000, 2000, 2500, 3000]\n",
        "for i in range(len(l)):\n",
        "  tree = DecisionTreeClassifier(min_samples_leaf=l[i])\n",
        "  tree.fit(X, y_pred)\n",
        "  if not os.path.isdir('./tree_'+name):\n",
        "    os.mkdir('./tree_'+name)\n",
        "  nodes = visualize(tree, './tree_'+name+'/tree' + str(i) + '.pdf',False,list(df.columns.values), class_names=list(cat_index.keys()))\n",
        "  acc = accuracy_score(tree.predict(X_test), [np.argmax(x) for x in Y_test])\n",
        "  leaf_indices = tree.apply(X)\n",
        "  leaf_counts = np.bincount(leaf_indices)\n",
        "  leaf_i = np.arange(tree.tree_.node_count)\n",
        "  apl = np.dot(leaf_i, leaf_counts) / float(X.shape[0])\n",
        "  if APL_mode == 1:\n",
        "    apl = np.sum(tree.decision_path(X)) / float(X.shape[0])\n",
        "  print('tree'+ str(i) + ': accuracy {:.5f}; number of nodes '.format(acc) + str(nodes) + '; APL {:.2f}'.format(apl))\n",
        "  log = open('./tree_'+name+'/log.txt', 'a')\n",
        "  log.write('tree'+ str(i) + ': accuracy {:.5f}; number of nodes '.format(acc) + str(nodes) + '; APL {:.2f}\\n'.format(apl))\n",
        "  log.close()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tree0: accuracy 0.94873; number of nodes 298; APL 188.57\n",
            "tree1: accuracy 0.94036; number of nodes 286; APL 184.29\n",
            "tree2: accuracy 0.94764; number of nodes 248; APL 158.14\n",
            "tree3: accuracy 0.94582; number of nodes 224; APL 143.06\n",
            "tree4: accuracy 0.96145; number of nodes 214; APL 138.23\n",
            "tree5: accuracy 0.94945; number of nodes 198; APL 126.49\n",
            "tree6: accuracy 0.95600; number of nodes 190; APL 123.02\n",
            "tree7: accuracy 0.94509; number of nodes 182; APL 118.06\n",
            "tree8: accuracy 0.95236; number of nodes 180; APL 116.69\n",
            "tree9: accuracy 0.94873; number of nodes 178; APL 115.04\n",
            "tree10: accuracy 0.95491; number of nodes 170; APL 111.11\n",
            "tree11: accuracy 0.95164; number of nodes 162; APL 104.95\n",
            "tree12: accuracy 0.94945; number of nodes 150; APL 96.63\n",
            "tree13: accuracy 0.95164; number of nodes 148; APL 95.69\n",
            "tree14: accuracy 0.95636; number of nodes 142; APL 91.04\n",
            "tree15: accuracy 0.96400; number of nodes 132; APL 83.03\n",
            "tree16: accuracy 0.96873; number of nodes 122; APL 75.57\n",
            "tree17: accuracy 0.96036; number of nodes 104; APL 62.92\n",
            "tree18: accuracy 0.94945; number of nodes 98; APL 59.57\n",
            "tree19: accuracy 0.93200; number of nodes 76; APL 45.61\n",
            "tree20: accuracy 0.90836; number of nodes 64; APL 37.81\n",
            "tree21: accuracy 0.89164; number of nodes 46; APL 26.86\n",
            "tree22: accuracy 0.89927; number of nodes 44; APL 25.46\n",
            "tree23: accuracy 0.89236; number of nodes 40; APL 22.64\n",
            "tree24: accuracy 0.91018; number of nodes 34; APL 19.03\n",
            "tree25: accuracy 0.85200; number of nodes 30; APL 16.27\n",
            "tree26: accuracy 0.84291; number of nodes 28; APL 15.95\n",
            "tree27: accuracy 0.78327; number of nodes 26; APL 14.20\n",
            "tree28: accuracy 0.88327; number of nodes 18; APL 9.55\n",
            "tree29: accuracy 0.78073; number of nodes 10; APL 5.08\n",
            "tree30: accuracy 0.84327; number of nodes 6; APL 2.50\n",
            "tree31: accuracy 0.59709; number of nodes 4; APL 1.58\n",
            "tree32: accuracy 0.59709; number of nodes 4; APL 1.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P5SDqg6Flxa2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Draw the graph"
      ]
    },
    {
      "metadata": {
        "id": "qjFYlgvvQuah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "84c8e36c-9084-46e8-abed-42fe5398cc11"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "drawtest()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAHACAYAAACsx95yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nV4W1TWTeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}